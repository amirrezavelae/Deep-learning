{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hr_Fa4aaAVDH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn.functional import relu\n",
        "from collections import defaultdict\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8EBcpJ_CLjh",
        "outputId": "5aabe7dd-d6eb-40ca-83fb-c94f71747b3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-23 19:37:06--  http://images.cocodataset.org/zips/train2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 16.182.106.113, 3.5.28.254, 3.5.29.82, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|16.182.106.113|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19336861798 (18G) [application/zip]\n",
            "Saving to: ‘train2017.zip’\n",
            "\n",
            "train2017.zip       100%[===================>]  18.01G  47.1MB/s    in 6m 49s  \n",
            "\n",
            "2023-12-23 19:43:56 (45.1 MB/s) - ‘train2017.zip’ saved [19336861798/19336861798]\n",
            "\n",
            "--2023-12-23 19:43:56--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.96.212, 54.231.133.241, 52.216.44.241, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.96.212|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  47.0MB/s    in 18s     \n",
            "\n",
            "2023-12-23 19:44:14 (44.2 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n",
            "\n",
            "--2023-12-23 19:44:14--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.221.233, 52.217.34.4, 52.217.137.97, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.221.233|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2017.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  46.8MB/s    in 5.6s    \n",
            "\n",
            "2023-12-23 19:44:19 (43.0 MB/s) - ‘annotations_trainval2017.zip’ saved [252907541/252907541]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://images.cocodataset.org/zips/train2017.zip\n",
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdH9kcChLcsp"
      },
      "source": [
        "Unzip the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SMo4KgDLcTn",
        "outputId": "513c67a1-80e9-4469-f0df-d7f483a31e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train2017.zip\n",
            "replace train2017/000000147328.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip \\*.zip\n",
        "!rm *.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiSSMaqKNZqd",
        "outputId": "ef821efe-075d-4d5c-eb43-95176aa64f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=22.94s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.67s)\n",
            "creating index...\n",
            "index created!\n",
            "Number of training samples: 18000\n",
            "Number of validation samples: 2000\n"
          ]
        }
      ],
      "source": [
        "# Define transformations\n",
        "import torchvision.transforms as transforms\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize((224, 224)),\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load training data\n",
        "train_data = datasets.CocoDetection(\n",
        "  root=\"./train2017\",\n",
        "  annFile=\"./annotations/instances_train2017.json\",\n",
        "  transform=transform\n",
        ")\n",
        "\n",
        "# Take subset for debugging\n",
        "train_data = torch.utils.data.Subset(train_data, list(range(18000)))\n",
        "\n",
        "# Load validation data\n",
        "val_data = datasets.CocoDetection(\n",
        "  root=\"./val2017\",\n",
        "  annFile=\"./annotations/instances_val2017.json\",\n",
        "  transform=transform\n",
        ")\n",
        "\n",
        "# Take subset for debugging\n",
        "val_data = torch.utils.data.Subset(val_data, list(range(2000)))\n",
        "\n",
        "print(f'Number of training samples: {len(train_data)}')\n",
        "print(f'Number of validation samples: {len(val_data)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hw2UzlUmLCwX"
      },
      "outputs": [],
      "source": [
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, num_objects, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.num_objects = num_objects\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, targets = self.dataset[index]\n",
        "\n",
        "        # Convert target to one-hot encoding\n",
        "        target_onehot = torch.zeros(self.num_objects)\n",
        "        for obj in targets:\n",
        "            target_onehot[obj['category_id']-1] = 1\n",
        "\n",
        "        return image, target_onehot\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EwOf2EKeCK2R"
      },
      "outputs": [],
      "source": [
        "# define train dataset\n",
        "train_dataset = CocoDataset(90,train_data)\n",
        "# define validation dataset\n",
        "val_dataset = CocoDataset(90,val_data)\n",
        "\n",
        "batch_size = 32\n",
        "# Create a data loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "gD_xLeJIAdpz"
      },
      "outputs": [],
      "source": [
        "class DeformableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, deformable_groups=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self.deformable_groups = deformable_groups\n",
        "\n",
        "        self.offset_conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            deformable_groups * 2,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation\n",
        "        )\n",
        "\n",
        "        self.modulator = nn.Conv2d(\n",
        "            in_channels,\n",
        "            deformable_groups,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation\n",
        "        )\n",
        "\n",
        "        self.feature_conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            dilation=dilation\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get deformable offsets\n",
        "        offsets = self.offset_conv(x)\n",
        "\n",
        "        # Get modulation masks\n",
        "        masks = torch.sigmoid(self.modulator(x))\n",
        "\n",
        "        # Deform the input features\n",
        "        x = self.deform_features(x, offsets, masks)\n",
        "\n",
        "        # Convolve deformed features\n",
        "        x = self.feature_conv(x)\n",
        "        return x\n",
        "\n",
        "    def deform_features(self, x, offsets, masks):\n",
        "        \"\"\"Deform the input features with offsets and modulation.\"\"\"\n",
        "        N, C, H, W = x.shape\n",
        "        tensor_mesh = self.get_mesh_grid(H, W, x.device)\n",
        "        tensor_mesh = tensor_mesh.reshape(2, H, W)\n",
        "\n",
        "        grid_y , grid_x = tensor_mesh[0, :,: ], tensor_mesh[1, :, :]\n",
        "\n",
        "        # Add offsets to sampling grid\n",
        "        grid_y = grid_y + offsets[:, 0, :, :]\n",
        "        grid_x = grid_x + offsets[:, 1, :, :]\n",
        "\n",
        "        sampling_locs = torch.stack((grid_x, grid_y), dim=-1)\n",
        "\n",
        "        # Interpolate features\n",
        "        x = nn.functional.grid_sample(\n",
        "            x, sampling_locs, mode='bilinear', padding_mode='zeros', align_corners=False\n",
        "        )\n",
        "\n",
        "        # Modulate features\n",
        "        x = x * masks\n",
        "        return x\n",
        "\n",
        "    def get_mesh_grid(self, H, W, device):\n",
        "        \"\"\"Generate mesh grid for sampling.\"\"\"\n",
        "        y, x = torch.meshgrid(torch.arange(H), torch.arange(W))\n",
        "        y = (2*y/(H-1) - 1).to(device)\n",
        "        x = (2*x/(W-1) - 1).to(device)\n",
        "        mesh_grid = torch.stack((x, y), dim=-1)\n",
        "\n",
        "        return mesh_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tRe3g6igPHtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc2f0eaf-402b-4689-8536-aa756ec7eedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 64, 224, 224])\n"
          ]
        }
      ],
      "source": [
        "deformable_conv = DeformableConv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, deformable_groups=1)\n",
        "test = torch.randn(6, 32, 224, 224)\n",
        "out = deformable_conv(test)\n",
        "print(out.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "amj21zMdVtma"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, deformable=False):\n",
        "        super().__init__()\n",
        "        conv = DeformableConv2d if deformable else nn.Conv2d\n",
        "\n",
        "        self.conv1 = conv(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.conv2 = conv(out_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ObjectDetector(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class, deformable=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block1 = Block(3, 64)\n",
        "        self.block2 = Block(64, 128, deformable=deformable)\n",
        "        self.block3 = Block(128, 256, deformable=deformable)\n",
        "        self.block4 = Block(256, 512, deformable=deformable)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(512, n_class, 1)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(17640, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, n_class)\n",
        "        )\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(n_class)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        x = self.block4(x)\n",
        "\n",
        "        x = self.out_conv(x)\n",
        "        x = self.classifier(x.view(x.size(0), -1))\n",
        "        x = self.bn(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "n607_1lSV14Y"
      },
      "outputs": [],
      "source": [
        "# number of classes\n",
        "num_class = 90\n",
        "# set device to be cuda\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# define model\n",
        "model1 = ObjectDetector(num_class).to(device)\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
        "\n",
        "# define learning rate scheduler\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "# set criterion to be BCEWithLogitsLoss\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "202XKmRqV30g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ca6c87-8271-4300-e4a4-d55040ac0aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Train loss: 0.9114\tVal loss: 0.8687\n",
            "Epoch 2/10\n",
            "Train loss: 0.8463\tVal loss: 0.8233\n",
            "Epoch 3/10\n",
            "Train loss: 0.8007\tVal loss: 0.7846\n",
            "Epoch 4/10\n",
            "Train loss: 0.7673\tVal loss: 0.7594\n",
            "Epoch 5/10\n",
            "Train loss: 0.7460\tVal loss: 0.7390\n",
            "Epoch 6/10\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "best_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model1.state_dict())\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "  # Training loop\n",
        "  model1.train()\n",
        "  train_loss = 0.0\n",
        "  for inputs, targets in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    outputs = model1(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "  train_loss /= len(train_loader.dataset)\n",
        "\n",
        "  # Validation loop\n",
        "  model1.eval()\n",
        "  val_loss = 0.0\n",
        "  for inputs, targets in val_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    outputs = model1(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "  val_loss /= len(val_loader.dataset)\n",
        "\n",
        "  # Print progress\n",
        "  print(f'Train loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}')\n",
        "\n",
        "  # Save best model\n",
        "  if val_loss < best_loss:\n",
        "    best_loss = val_loss\n",
        "    best_model_wts = copy.deepcopy(model1.state_dict())\n",
        "\n",
        "# Load best model weights\n",
        "model1.load_state_dict(best_model_wts)\n",
        "torch.save(model1.state_dict(), 'best_model1.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It took 68 minutes for 20 epochs so i'm gonna set eppochs to 10 :)"
      ],
      "metadata": {
        "id": "mVfpnZjQHFIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numba\n",
        "\n",
        "from numba import cuda\n",
        "device = cuda.get_current_device()\n",
        "device.reset()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XmS6DBVaAiZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvjh53EjWzey"
      },
      "outputs": [],
      "source": [
        "model2 = ObjectDetector(num_class,deformable=True).to(device)\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_loss = float('inf')\n",
        "best_model_wts = copy.deepcopy(model2.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "  # Training loop\n",
        "  model2.train()\n",
        "  train_loss = 0.0\n",
        "  for inputs, targets in train_loader:\n",
        "    optimizer.zero_grad()\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    outputs = model2(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "  train_loss /= len(train_loader.dataset)\n",
        "\n",
        "  # Validation loop\n",
        "  model2.eval()\n",
        "  val_loss = 0.0\n",
        "  for inputs, targets in val_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    outputs = model2(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "  val_loss /= len(val_loader.dataset)\n",
        "\n",
        "  # Print progress\n",
        "  print(f'Train loss: {train_loss:.4f}\\tVal loss: {val_loss:.4f}')\n",
        "\n",
        "  # Save best model\n",
        "  if val_loss < best_loss:\n",
        "    best_loss = val_loss\n",
        "    best_model_wts = copy.deepcopy(model2.state_dict())\n",
        "\n",
        "# Load best model weights\n",
        "model2.load_state_dict(best_model_wts)\n",
        "torch.save(model2.state_dict(), 'best_model2.pt')"
      ],
      "metadata": {
        "id": "PBjic8OcyW8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best model weights\n",
        "model1.load_state_dict(torch.load('best_model1.pth'))\n",
        "model2.load_state_dict(torch.load('best_model2.pth'))\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model1.eval()\n",
        "model2.eval()\n",
        "\n",
        "# Compute precision, recall, F1 score\n",
        "y_true = []\n",
        "y_pred1 = []\n",
        "y_pred2 = []\n",
        "\n",
        "for inputs, targets in tqdm(test_loader):\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        outputs1 = model1(inputs)\n",
        "        outputs2 = model2(inputs)\n",
        "\n",
        "        y_true.extend(targets.cpu().numpy().tolist())\n",
        "        y_pred1.extend(outputs1.cpu().numpy().tolist())\n",
        "        y_pred2.extend(outputs2.cpu().numpy().tolist())\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred1 = np.array(y_pred1)\n",
        "y_pred2 = np.array(y_pred2)\n",
        "\n",
        "y_pred1[y_pred1 >= 0.5] = 1\n",
        "y_pred1[y_pred1 < 0.5] = 0\n",
        "\n",
        "y_pred2[y_pred2 >= 0.5] = 1\n",
        "y_pred2[y_pred2 < 0.5] = 0\n",
        "\n",
        "precision1, recall1, f1_score1, _ = precision_recall_fscore_support(y_true, y_pred1, average='micro')\n",
        "precision2, recall2, f1_score2, _ = precision_recall_fscore_support(y_true, y_pred2, average='micro')\n",
        "\n",
        "print(f'Precision without deformable convolutions: {precision1:.4f}')\n",
        "print(f'Recall without deformable convolutions: {recall1:.4f}')\n",
        "print(f'F1 score without deformable convolutions: {f1_score1:.4f}')\n",
        "print()\n",
        "print(f'Precision with deformable convolutions: {precision2:.4f}')\n",
        "print(f'Recall with deformable convolutions: {recall2:.4f}')\n",
        "print(f'F1 score with deformable convolutions: {f1_score2:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jGEYhhJ6JeAa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}