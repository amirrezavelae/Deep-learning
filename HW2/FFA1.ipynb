{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tensorflow.compiler.tf2xla.python import xla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"4 Random Training samples and labels\")\n",
    "idx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n",
    "\n",
    "img1 = (x_train[idx1], y_train[idx1])\n",
    "img2 = (x_train[idx2], y_train[idx2])\n",
    "img3 = (x_train[idx3], y_train[idx3])\n",
    "img4 = (x_train[idx4], y_train[idx4])\n",
    "\n",
    "imgs = [img1, img2, img3, img4]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for idx, item in enumerate(imgs):\n",
    "    image, label = item[0], item[1]\n",
    "    plt.subplot(2, 2, idx + 1)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.title(f\"Label : {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFDense(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
    "    Forward-Forward network internally for use.\n",
    "    This layer must be used in conjunction with the `FFNetwork` model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        optimizer,\n",
    "        loss_metric,\n",
    "        num_epochs=50,\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        kernel_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dense = keras.layers.Dense(\n",
    "            units=units,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "        )\n",
    "        self.relu = keras.layers.ReLU()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_metric = loss_metric\n",
    "        self.threshold = 1.5\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    # We perform a normalization step before we run the input through the Dense\n",
    "    # layer.\n",
    "\n",
    "    def call(self, x):\n",
    "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
    "        x_norm = x_norm + 1e-4\n",
    "        x_dir = x / x_norm\n",
    "        res = self.dense(x_dir)\n",
    "        return self.relu(res)\n",
    "\n",
    "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
    "    # operation and then get a Mean Square value for all positive and negative\n",
    "    # samples respectively.\n",
    "    # The custom loss function finds the distance between the Mean-squared\n",
    "    # result and the threshold value we set (a hyperparameter) that will define\n",
    "    # whether the prediction is positive or negative in nature. Once the loss is\n",
    "    # calculated, we get a mean across the entire batch combined and perform a\n",
    "    # gradient calculation and optimization step. This does not technically\n",
    "    # qualify as backpropagation since there is no gradient being\n",
    "    # sent to any previous layer and is completely local in nature.\n",
    "\n",
    "    def forward_forward(self, x_pos, x_neg):\n",
    "        for i in range(self.num_epochs):\n",
    "            with tf.GradientTape() as tape:\n",
    "                g_pos = tf.math.reduce_mean(tf.math.pow(self.call(x_pos), 2), 1)\n",
    "                g_neg = tf.math.reduce_mean(tf.math.pow(self.call(x_neg), 2), 1)\n",
    "\n",
    "                loss = tf.math.log(\n",
    "                    1\n",
    "                    + tf.math.exp(\n",
    "                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n",
    "                    )\n",
    "                )\n",
    "                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n",
    "                self.loss_metric.update_state([mean_loss])\n",
    "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
    "        return (\n",
    "            tf.stop_gradient(self.call(x_pos)),\n",
    "            tf.stop_gradient(self.call(x_neg)),\n",
    "            self.loss_metric.result(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNetwork(keras.Model):\n",
    "    \"\"\"\n",
    "    A [`keras.Model`](/api/models/model#model-class) that supports a `FFDense` network creation. This model\n",
    "    can work for any kind of classification task. It has an internal\n",
    "    implementation with some details specific to the MNIST dataset which can be\n",
    "    changed as per the use-case.\n",
    "    \"\"\"\n",
    "\n",
    "    # Since each layer runs gradient-calculation and optimization locally, each\n",
    "    # layer has its own optimizer that we pass. As a standard choice, we pass\n",
    "    # the `Adam` optimizer with a default learning rate of 0.03 as that was\n",
    "    # found to be the best rate after experimentation.\n",
    "    # Loss is tracked using `loss_var` and `loss_count` variables.\n",
    "    # Use legacy optimizer for Layer Optimizer to fix issue\n",
    "    # https://github.com/keras-team/keras-io/issues/1241\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims,\n",
    "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.layer_optimizer = layer_optimizer\n",
    "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
    "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
    "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
    "        for d in range(len(dims) - 1):\n",
    "            self.layer_list += [\n",
    "                FFDense(\n",
    "                    dims[d + 1],\n",
    "                    optimizer=self.layer_optimizer,\n",
    "                    loss_metric=keras.metrics.Mean(),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "    # This function makes a dynamic change to the image wherein the labels are\n",
    "    # put on top of the original image (for this example, as MNIST has 10\n",
    "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
    "    # function returns the original data tensor with the first 10 pixels being\n",
    "    # a pixel-based one-hot representation of the labels.\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def overlay_y_on_x(self, data):\n",
    "        X_sample, y_sample = data\n",
    "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
    "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
    "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
    "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
    "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
    "        return X_sample, y_sample\n",
    "\n",
    "    # A custom `predict_one_sample` performs predictions by passing the images\n",
    "    # through the network, measures the results produced by each layer (i.e.\n",
    "    # how high/low the output values are with respect to the set threshold for\n",
    "    # each label) and then simply finding the label with the highest values.\n",
    "    # In such a case, the images are tested for their 'goodness' with all\n",
    "    # labels.\n",
    "\n",
    "    @tf.function(reduce_retracing=True)\n",
    "    def predict_one_sample(self, x):\n",
    "        goodness_per_label = []\n",
    "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
    "        for label in range(10):\n",
    "            h, label = self.overlay_y_on_x(data=(x, label))\n",
    "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
    "            goodness = []\n",
    "            for layer_idx in range(1, len(self.layer_list)):\n",
    "                layer = self.layer_list[layer_idx]\n",
    "                h = layer(h)\n",
    "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
    "            goodness_per_label += [\n",
    "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
    "            ]\n",
    "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
    "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
    "\n",
    "    def predict(self, data):\n",
    "        x = data\n",
    "        preds = list()\n",
    "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
    "        return np.asarray(preds, dtype=int)\n",
    "\n",
    "    # This custom `train_step` function overrides the internal `train_step`\n",
    "    # implementation. We take all the input image tensors, flatten them and\n",
    "    # subsequently produce positive and negative samples on the images.\n",
    "    # A positive sample is an image that has the right label encoded on it with\n",
    "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
    "    # erroneous label present on it.\n",
    "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
    "    # the Forward-Forward computation on it. The returned loss is the final\n",
    "    # loss value over all the layers.\n",
    "\n",
    "    @tf.function(jit_compile=True)\n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "\n",
    "        # Flatten op\n",
    "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
    "\n",
    "        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
    "\n",
    "        random_y = tf.random.shuffle(y)\n",
    "        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
    "\n",
    "        h_pos, h_neg = x_pos, x_neg\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, FFDense):\n",
    "                print(f\"Training layer {idx+1} now : \")\n",
    "                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n",
    "                self.loss_var.assign_add(loss)\n",
    "                self.loss_count.assign_add(1.0)\n",
    "            else:\n",
    "                print(f\"Passing layer {idx+1} now : \")\n",
    "                x = layer(x)\n",
    "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
    "        return {\"FinalLoss\": mean_res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype(float) / 255\n",
    "x_test = x_test.astype(float) / 255\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "train_dataset = train_dataset.batch(60000)\n",
    "test_dataset = test_dataset.batch(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FFNetwork(dims=[784, 500, 500])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
    "    loss=\"mse\",\n",
    "    jit_compile=True,\n",
    "    metrics=[keras.metrics.Mean()],\n",
    ")\n",
    "\n",
    "epochs = 250\n",
    "history = model.fit(train_dataset, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tf.convert_to_tensor(x_test))\n",
    "\n",
    "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
    "\n",
    "results = accuracy_score(preds, y_test)\n",
    "\n",
    "print(f\"Test Accuracy score : {results*100}%\")\n",
    "\n",
    "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
    "plt.title(\"Loss over training\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
